@article{chen2023dialogue,
  title={Dialogue relation extraction with document-level heterogeneous graph attention networks},
  author={Chen, Hui and Hong, Pengfei and Han, Wei and Majumder, Navonil and Poria, Soujanya},
  journal={Cognitive Computation},
  pages={1--10},
  year={2023},
  publisher={Springer}
}

@article{wei2302zero,
  title={Zero-shot information extraction via chatting with chatgpt. arXiv 2023},
  author={Wei, X and Cui, X and Cheng, N and Wang, X and Zhang, X and Huang, S and Xie, P and Xu, J and Chen, Y and Zhang, M and others},
  journal={arXiv preprint arXiv:2302.10205}
}

@article{zheng2023sentence,
  title={Sentence-Level Relation Extraction via Contrastive Learning with Descriptive Relation Prompts},
  author={Zheng, Jiewen and Chen, Ze},
  journal={arXiv preprint arXiv:2304.04935},
  year={2023}
}

@inproceedings{zhang2023rdrs,
  title={RDRS: Represent Document-level Relation with Sentence-level Relation by Distant Supervision},
  author={Zhang, Tingrui and Liu, Guiquan},
  booktitle={2023 IEEE International Conference on Control, Electronics and Computer Technology (ICCECT)},
  pages={1485--1490},
  year={2023},
  organization={IEEE}
}

@article{baek2022enhancing,
  title={Enhancing Targeted Minority Class Prediction in Sentence-Level Relation Extraction},
  author={Baek, Hyeong-Ryeol and Choi, Yong-Suk},
  journal={Sensors},
  volume={22},
  number={13},
  pages={4911},
  year={2022},
  publisher={MDPI}
}

@article{park4272160effective,
  title={Effective Sentence-Level Relation Extraction Model Using Entity-Centric Dependency Tree},
  author={Park, Seongsik and Kim, Harksoo},
  journal={Available at SSRN 4272160}
}

@article{che2022label,
  title={Label correlation in multi-label classification using local attribute reductions with fuzzy rough sets},
  author={Che, Xiaoya and Chen, Degang and Mi, Jusheng},
  journal={Fuzzy Sets and Systems},
  volume={426},
  pages={121--144},
  year={2022},
  publisher={Elsevier}
}

@inproceedings{huang2021entity,
  title={Entity and evidence guided document-level relation extraction},
  author={Huang, Kevin and Qi, Peng and Wang, Guangtao and Ma, Tengyu and Huang, Jing},
  booktitle={Proceedings of the 6th Workshop on Representation Learning for NLP (RepL4NLP-2021)},
  pages={307--315},
  year={2021}
}

@inproceedings{huang-etal-2021-three,
    title = "Three Sentences Are All You Need: Local Path Enhanced Document Relation Extraction",
    author = "Huang, Quzhe  and
      Zhu, Shengqi  and
      Feng, Yansong  and
      Ye, Yuan  and
      Lai, Yuxuan  and
      Zhao, Dongyan",
    editor = "Zong, Chengqing  and
      Xia, Fei  and
      Li, Wenjie  and
      Navigli, Roberto",
    booktitle = "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 2: Short Papers)",
    month = aug,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.acl-short.126",
    doi = "10.18653/v1/2021.acl-short.126",
    pages = "998--1004",
    abstract = "Document-level Relation Extraction (RE) is a more challenging task than sentence RE as it often requires reasoning over multiple sentences. Yet, human annotators usually use a small number of sentences to identify the relationship between a given entity pair. In this paper, we present an embarrassingly simple but effective method to heuristically select evidence sentences for document-level RE, which can be easily combined with BiLSTM to achieve good performance on benchmark datasets, even better than fancy graph neural network based methods. We have released our code at \url{https://github.com/AndrewZhe/Three-Sentences-Are-All-You-Need}.",
}

@article{10.1145/3520082,
author = {Shang, Yu-Ming and Huang, Heyan and Sun, Xin and Wei, Wei and Mao, Xian-Ling},
title = {Learning Relation Ties with a Force-Directed Graph in Distant Supervised Relation Extraction},
year = {2023},
issue_date = {January 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {41},
number = {1},
issn = {1046-8188},
url = {https://doi.org/10.1145/3520082},
doi = {10.1145/3520082},
abstract = {Relation ties, defined as the correlation and mutual exclusion between different relations, are critical for distant supervised relation extraction. Previous studies usually obtain this property by greedily learning the local connections between relations. However, they are essentially limited because of failing to capture the global topology structure of relation ties and may easily fall into a locally optimal solution. To address this issue, we propose a novel force-directed graph to comprehensively learn relation ties. Specifically, we first construct a graph according to the global co-occurrence of all relations. Then, we borrow the idea of Coulomb’s law from physics and introduce the concept of attractive force and repulsive force into this graph to learn correlation and mutual exclusion between relations. Finally, the obtained relation representations are applied as an inter-dependent relation classifier. Extensive experimental results demonstrate that our method is capable of modeling global correlation and mutual exclusion between relations, and outperforms the state-of-the-art baselines. In addition, the proposed force-directed graph can be used as a module to augment existing relation extraction systems and improve their performance.},
journal = {ACM Trans. Inf. Syst.},
month = {jan},
articleno = {10},
numpages = {23},
keywords = {force-directed graph, relation ties, relation extraction, Distant supervision}
}

@inproceedings{10.1145/3511808.3557313,
author = {Xu, Tianyu and Hua, Wen and Qu, Jianfeng and Li, Zhixu and Xu, Jiajie and Liu, An and Zhao, Lei},
title = {Evidence-aware Document-level Relation Extraction},
year = {2022},
isbn = {9781450392365},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3511808.3557313},
doi = {10.1145/3511808.3557313},
abstract = {Document-level Relation Extraction (RE) is a promising task aiming at identifying relations of multiple entity pairs in a document. However, in most cases, a relational fact can be expressed enough via a small subset of sentences from the document, namely evidence sentence. Moreover, there often exist strong semantic correlations between evidence sentences that collaborate together to describe a specific relation. To address these challenges, we propose a novel evidence-aware model for document-level RE. Particularly, we formulate evidence sentence selection as a sequential decision problem through a crafted reinforcement learning mechanism. Considering the explosive search space of our agent, an efficient path searching strategy is executed on the converted document graph to heuristically obtain hopeful sentences and feed them to reinforcement learning. Finally, each entity pair owns a customized-filtered document for further inferring the relation between them. We conduct various experiments on two document-level RE benchmarks and achieve a remarkable improvement over previous competitive baselines, verifying the effectiveness of our method.},
booktitle = {Proceedings of the 31st ACM International Conference on Information \& Knowledge Management},
pages = {2311–2320},
numpages = {10},
keywords = {reinforcement learning, evidence extraction, document-level relation extraction},
location = {Atlanta, GA, USA},
series = {CIKM '22}
}

@inproceedings{10.1145/3502223.3502245,
author = {Zhao, Jiehao and Duan, Guiduo and Huang, Tianxi},
title = {Pre-classification Supporting Reasoning for Document-level Relation Extraction},
year = {2022},
isbn = {9781450395656},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3502223.3502245},
doi = {10.1145/3502223.3502245},
abstract = {The document-level relation extraction task aims to extract relational triples from a document consisting of multiple sentences. Most previous models focus on modeling the dependency between the entities and neglect the reasoning mechanism. Some other models construct paths implicitly between co-sentence entities to find semantic relations. However, they ignore that there are interactions between different triples, especially some triples play an import role in predicting others. In this short research paper, we propose a new two stage framework PCSR(Pre-classification Supporting Reasoning) which captures the interactions between triples and utilizes these information for reasoning. Specifically, we make a pre-classification for each entity pair in the first stage. Then we aggregate the embeddings of predicted triples to enhance entity representation and make a new classification. Since the second classification could find triples missed in the first stage, we take the result as the supplement of the prior one. Experiments on DocRED show that our method achieves an F1 score of 62.11. Compared with the previous state-of-the-art model, our model increase by 0.81 on the test set, which demonstrates the effectiveness of our reasoning mechanism.},
booktitle = {Proceedings of the 10th International Joint Conference on Knowledge Graphs},
pages = {156–160},
numpages = {5},
keywords = {BERT, Document-level Relation Extraction, Information Extraction, Reasoning Mechanism},
location = {Virtual Event, Thailand},
series = {IJCKG '21}
}

@inproceedings{ijcai2021p551,
  title     = {Document-level Relation Extraction as Semantic Segmentation},
  author    = {Zhang, Ningyu and Chen, Xiang and Xie, Xin and Deng, Shumin and Tan, Chuanqi and Chen, Mosha and Huang, Fei and Si, Luo and Chen, Huajun},
  booktitle = {Proceedings of the Thirtieth International Joint Conference on
               Artificial Intelligence, {IJCAI-21}},
  publisher = {International Joint Conferences on Artificial Intelligence Organization},
  editor    = {Zhi-Hua Zhou},
  pages     = {3999--4006},
  year      = {2021},
  month     = {8},
  note      = {Main Track},
  doi       = {10.24963/ijcai.2021/551},
  url       = {https://doi.org/10.24963/ijcai.2021/551},
}

@inproceedings{ma-etal-2023-dreeam,
    title = "{DREEAM}: Guiding Attention with Evidence for Improving Document-Level Relation Extraction",
    author = "Ma, Youmi  and
      Wang, An  and
      Okazaki, Naoaki",
    editor = "Vlachos, Andreas  and
      Augenstein, Isabelle",
    booktitle = "Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics",
    month = may,
    year = "2023",
    address = "Dubrovnik, Croatia",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.eacl-main.145",
    doi = "10.18653/v1/2023.eacl-main.145",
    pages = "1971--1983",
    abstract = "Document-level relation extraction (DocRE) is the task of identifying all relations between each entity pair in a document. Evidence, defined as sentences containing clues for the relationship between an entity pair, has been shown to help DocRE systems focus on relevant texts, thus improving relation extraction. However, evidence retrieval (ER) in DocRE faces two major issues: high memory consumption and limited availability of annotations. This work aims at addressing these issues to improve the usage of ER in DocRE. First, we propose DREEAM, a memory-efficient approach that adopts evidence information as the supervisory signal, thereby guiding the attention modules of the DocRE system to assign high weights to evidence. Second, we propose a self-training strategy for DREEAM to learn ER from automatically-generated evidence on massive data without evidence annotations. Experimental results reveal that our approach exhibits state-of-the-art performance on the DocRED benchmark for both DocRE and ER. To the best of our knowledge, DREEAM is the first approach to employ ER self-training.",
}

@inproceedings{wan-etal-2021-dqn,
    title = "A {DQN}-based Approach to Finding Precise Evidences for Fact Verification",
    author = "Wan, Hai  and
      Chen, Haicheng  and
      Du, Jianfeng  and
      Luo, Weilin  and
      Ye, Rongzhen",
    editor = "Zong, Chengqing  and
      Xia, Fei  and
      Li, Wenjie  and
      Navigli, Roberto",
    booktitle = "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)",
    month = aug,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.acl-long.83",
    doi = "10.18653/v1/2021.acl-long.83",
    pages = "1030--1039",
    abstract = "Computing precise evidences, namely minimal sets of sentences that support or refute a given claim, rather than larger evidences is crucial in fact verification (FV), since larger evidences may contain conflicting pieces some of which support the claim while the other refute, thereby misleading FV. Despite being important, precise evidences are rarely studied by existing methods for FV. It is challenging to find precise evidences due to a large search space with lots of local optimums. Inspired by the strong exploration ability of the deep Q-learning network (DQN), we propose a DQN-based approach to retrieval of precise evidences. In addition, to tackle the label bias on Q-values computed by DQN, we design a post-processing strategy which seeks best thresholds for determining the true labels of computed evidences. Experimental results confirm the effectiveness of DQN in computing precise evidences and demonstrate improvements in achieving accurate claim verification.",
}

@ARTICLE{9684942,
  author={Cheng, Harry and Liao, Lizi and Hu, Linmei and Nie, Liqiang},
  journal={IEEE Transactions on Big Data}, 
  title={Multi-Relation Extraction via A Global-Local Graph Convolutional Network}, 
  year={2022},
  volume={8},
  number={6},
  pages={1716-1728},
  keywords={Feature extraction;Data mining;Convolutional codes;Games;Task analysis;Big Data;Semantics;Relation extraction;overlapping triplets;graph convolution;natural language processing},
  doi={10.1109/TBDATA.2022.3144151}}

  @inproceedings{wei-etal-2020-novel,
    title = "A Novel Cascade Binary Tagging Framework for Relational Triple Extraction",
    author = "Wei, Zhepei  and
      Su, Jianlin  and
      Wang, Yue  and
      Tian, Yuan  and
      Chang, Yi",
    editor = "Jurafsky, Dan  and
      Chai, Joyce  and
      Schluter, Natalie  and
      Tetreault, Joel",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.acl-main.136",
    doi = "10.18653/v1/2020.acl-main.136",
    pages = "1476--1488",
    abstract = "Extracting relational triples from unstructured text is crucial for large-scale knowledge graph construction. However, few existing works excel in solving the overlapping triple problem where multiple relational triples in the same sentence share the same entities. In this work, we introduce a fresh perspective to revisit the relational triple extraction task and propose a novel cascade binary tagging framework (CasRel) derived from a principled problem formulation. Instead of treating relations as discrete labels as in previous works, our new framework models relations as functions that map subjects to objects in a sentence, which naturally handles the overlapping problem. Experiments show that the CasRel framework already outperforms state-of-the-art methods even when its encoder module uses a randomly initialized BERT encoder, showing the power of the new tagging framework. It enjoys further performance boost when employing a pre-trained BERT encoder, outperforming the strongest baseline by 17.5 and 30.2 absolute gain in F1-score on two public datasets NYT and WebNLG, respectively. In-depth analysis on different scenarios of overlapping triples shows that the method delivers consistent performance gain across all these scenarios. The source code and data are released online.",
}

@inproceedings{zhang-etal-2021-multi-label-multi,
    title = "A Multi-label Multi-hop Relation Detection Model based on Relation-aware Sequence Generation",
    author = "Zhang, Linhai  and
      Zhou, Deyu  and
      Lin, Chao  and
      He, Yulan",
    editor = "Moens, Marie-Francine  and
      Huang, Xuanjing  and
      Specia, Lucia  and
      Yih, Scott Wen-tau",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2021",
    month = nov,
    year = "2021",
    address = "Punta Cana, Dominican Republic",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.findings-emnlp.404",
    doi = "10.18653/v1/2021.findings-emnlp.404",
    pages = "4713--4719",
    abstract = "Multi-hop relation detection in Knowledge Base Question Answering (KBQA) aims at retrieving the relation path starting from the topic entity to the answer node based on a given question, where the relation path may comprise multiple relations. Most of the existing methods treat it as a single-label learning problem while ignoring the fact that for some complex questions, there exist multiple correct relation paths in knowledge bases. Therefore, in this paper, multi-hop relation detection is considered as a multi-label learning problem. However, performing multi-label multi-hop relation detection is challenging since the numbers of both the labels and the hops are unknown. To tackle this challenge, multi-label multi-hop relation detection is formulated as a sequence generation task. A relation-aware sequence relation generation model is proposed to solve the problem in an end-to-end manner. Experimental results show the effectiveness of the proposed method for relation detection and KBQA.",
}

@inproceedings{singhania-etal-2023-extracting,
    title = "Extracting Multi-valued Relations from Language Models",
    author = "Singhania, Sneha  and
      Razniewski, Simon  and
      Weikum, Gerhard",
    editor = "Can, Burcu  and
      Mozes, Maximilian  and
      Cahyawijaya, Samuel  and
      Saphra, Naomi  and
      Kassner, Nora  and
      Ravfogel, Shauli  and
      Ravichander, Abhilasha  and
      Zhao, Chen  and
      Augenstein, Isabelle  and
      Rogers, Anna  and
      Cho, Kyunghyun  and
      Grefenstette, Edward  and
      Voita, Lena",
    booktitle = "Proceedings of the 8th Workshop on Representation Learning for NLP (RepL4NLP 2023)",
    month = jul,
    year = "2023",
    address = "Toronto, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.repl4nlp-1.12",
    doi = "10.18653/v1/2023.repl4nlp-1.12",
    pages = "139--154",
    abstract = "The widespread usage of latent language representations via pre-trained language models (LMs) suggests that they are a promising source of structured knowledge. However, existing methods focus only on a single object per subject-relation pair, even though often multiple objects are correct. To overcome this limitation, we analyze these representations for their potential to yield materialized multi-object relational knowledge. We formulate the problem as a rank-then-select task. For ranking candidate objects, we evaluate existing prompting techniques and propose new ones incorporating domain knowledge. Among the selection methods, we find that choosing objects with a likelihood above a learned relation-specific threshold gives a 49.5{\%} F1 score. Our results highlight the difficulty of employing LMs for the multi-valued slot-filling task, and pave the way for further research on extracting relational knowledge from latent language representations.",
}

@article{alqaaidi2023multiple,
  title={Multiple Relations Classification using Imbalanced Predictions Adaptation},
  author={Alqaaidi, Sakher Khalil and Bozorgi, Elika and Kochut, Krzysztof J},
  journal={arXiv preprint arXiv:2309.13718},
  year={2023}
}

@inproceedings{devlin-etal-2019-bert,
    title = "{BERT}: Pre-training of Deep Bidirectional Transformers for Language Understanding",
    author = "Devlin, Jacob  and
      Chang, Ming-Wei  and
      Lee, Kenton  and
      Toutanova, Kristina",
    editor = "Burstein, Jill  and
      Doran, Christy  and
      Solorio, Thamar",
    booktitle = "Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",
    month = jun,
    year = "2019",
    address = "Minneapolis, Minnesota",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/N19-1423",
    doi = "10.18653/v1/N19-1423",
    pages = "4171--4186",
    abstract = "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5 (7.7 point absolute improvement), MultiNLI accuracy to 86.7{\%} (4.6{\%} absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).",
}

@inproceedings{zhou2021document,
  title={Document-level relation extraction with adaptive thresholding and localized context pooling},
  author={Zhou, Wenxuan and Huang, Kevin and Ma, Tengyu and Huang, Jing},
  booktitle={Proceedings of the AAAI conference on artificial intelligence},
  volume={35},
  number={16},
  pages={14612--14620},
  year={2021}
}

@inproceedings{jia-etal-2019-document,
    title = "Document-Level N-ary Relation Extraction with Multiscale Representation Learning",
    author = "Jia, Robin  and
      Wong, Cliff  and
      Poon, Hoifung",
    editor = "Burstein, Jill  and
      Doran, Christy  and
      Solorio, Thamar",
    booktitle = "Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",
    month = jun,
    year = "2019",
    address = "Minneapolis, Minnesota",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/N19-1370",
    doi = "10.18653/v1/N19-1370",
    pages = "3693--3704",
    abstract = "Most information extraction methods focus on binary relations expressed within single sentences. In high-value domains, however, n-ary relations are of great demand (e.g., drug-gene-mutation interactions in precision oncology). Such relations often involve entity mentions that are far apart in the document, yet existing work on cross-sentence relation extraction is generally confined to small text spans (e.g., three consecutive sentences), which severely limits recall. In this paper, we propose a novel multiscale neural architecture for document-level n-ary relation extraction. Our system combines representations learned over various text spans throughout the document and across the subrelation hierarchy. Widening the system{'}s purview to the entire document maximizes potential recall. Moreover, by integrating weak signals across the document, multiscale modeling increases precision, even in the presence of noisy labels from distant supervision. Experiments on biomedical machine reading show that our approach substantially outperforms previous n-ary relation extraction methods.",
}

@inproceedings{tan-etal-2022-document,
    title = "Document-Level Relation Extraction with Adaptive Focal Loss and Knowledge Distillation",
    author = "Tan, Qingyu  and
      He, Ruidan  and
      Bing, Lidong  and
      Ng, Hwee Tou",
    editor = "Muresan, Smaranda  and
      Nakov, Preslav  and
      Villavicencio, Aline",
    booktitle = "Findings of the Association for Computational Linguistics: ACL 2022",
    month = may,
    year = "2022",
    address = "Dublin, Ireland",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.findings-acl.132",
    doi = "10.18653/v1/2022.findings-acl.132",
    pages = "1672--1681",
    abstract = "Document-level Relation Extraction (DocRE) is a more challenging task compared to its sentence-level counterpart. It aims to extract relations from multiple sentences at once. In this paper, we propose a semi-supervised framework for DocRE with three novel components. Firstly, we use an axial attention module for learning the interdependency among entity-pairs, which improves the performance on two-hop relations. Secondly, we propose an adaptive focal loss to tackle the class imbalance problem of DocRE. Lastly, we use knowledge distillation to overcome the differences between human annotated data and distantly supervised data. We conducted experiments on two DocRE datasets. Our model consistently outperforms strong baselines and its performance exceeds the previous SOTA by 1.36 F1 and 1.46 Ign{\_}F1 score on the DocRED leaderboard.",
}

@inproceedings{mintz-etal-2009-distant,
    title = "Distant supervision for relation extraction without labeled data",
    author = "Mintz, Mike  and
      Bills, Steven  and
      Snow, Rion  and
      Jurafsky, Daniel",
    editor = "Su, Keh-Yih  and
      Su, Jian  and
      Wiebe, Janyce  and
      Li, Haizhou",
    booktitle = "Proceedings of the Joint Conference of the 47th Annual Meeting of the {ACL} and the 4th International Joint Conference on Natural Language Processing of the {AFNLP}",
    month = aug,
    year = "2009",
    address = "Suntec, Singapore",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P09-1113",
    pages = "1003--1011",
}

@inproceedings{xie-etal-2022-eider,
    title = "Eider: Empowering Document-level Relation Extraction with Efficient Evidence Extraction and Inference-stage Fusion",
    author = "Xie, Yiqing  and
      Shen, Jiaming  and
      Li, Sha  and
      Mao, Yuning  and
      Han, Jiawei",
    editor = "Muresan, Smaranda  and
      Nakov, Preslav  and
      Villavicencio, Aline",
    booktitle = "Findings of the Association for Computational Linguistics: ACL 2022",
    month = may,
    year = "2022",
    address = "Dublin, Ireland",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.findings-acl.23",
    doi = "10.18653/v1/2022.findings-acl.23",
    pages = "257--268",
    abstract = "Document-level relation extraction (DocRE) aims to extract semantic relations among entity pairs in a document. Typical DocRE methods blindly take the full document as input, while a subset of the sentences in the document, noted as the evidence, are often sufficient for humans to predict the relation of an entity pair. In this paper, we propose an evidence-enhanced framework, Eider, that empowers DocRE by efficiently extracting evidence and effectively fusing the extracted evidence in inference. We first jointly train an RE model with a lightweight evidence extraction model, which is efficient in both memory and runtime. Empirically, even training the evidence model on silver labels constructed by our heuristic rules can lead to better RE performance. We further design a simple yet effective inference process that makes RE predictions on both extracted evidence and the full document, then fuses the predictions through a blending layer. This allows Eider to focus on important sentences while still having access to the complete information in the document. Extensive experiments show that Eider outperforms state-of-the-art methods on three benchmark datasets (e.g., by 1.37/1.26 Ign F1/F1 on DocRED).",
}

@inproceedings{xu2021document,
  title={Document-level relation extraction with reconstruction},
  author={Xu, Wang and Chen, Kehai and Zhao, Tiejun},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={35},
  number={16},
  pages={14167--14175},
  year={2021}
}

@inproceedings{tan-etal-2022-revisiting,
    title = "Revisiting {D}oc{RED} - Addressing the False Negative Problem in Relation Extraction",
    author = "Tan, Qingyu  and
      Xu, Lu  and
      Bing, Lidong  and
      Ng, Hwee Tou  and
      Aljunied, Sharifah Mahani",
    editor = "Goldberg, Yoav  and
      Kozareva, Zornitsa  and
      Zhang, Yue",
    booktitle = "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing",
    month = dec,
    year = "2022",
    address = "Abu Dhabi, United Arab Emirates",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.emnlp-main.580",
    doi = "10.18653/v1/2022.emnlp-main.580",
    pages = "8472--8487",
    abstract = "The DocRED dataset is one of the most popular and widely used benchmarks for document-level relation extraction (RE). It adopts a recommend-revise annotation scheme so as to have a large-scale annotated dataset. However, we find that the annotation of DocRED is incomplete, i.e., false negative samples are prevalent. We analyze the causes and effects of the overwhelming false negative problem in the DocRED dataset. To address the shortcoming, we re-annotate 4,053 documents in the DocRED dataset by adding the missed relation triples back to the original DocRED. We name our revised DocRED dataset Re-DocRED. We conduct extensive experiments with state-of-the-art neural models on both datasets, and the experimental results show that the models trained and evaluated on our Re-DocRED achieve performance improvements of around 13 F1 points. Moreover, we conduct a comprehensive analysis to identify the potential areas for further improvement.",
}

@article{zhu2024fcds,
  title={FCDS: Fusing Constituency and Dependency Syntax into Document-Level Relation Extraction},
  author={Zhu, Xudong and Kang, Zhao and Hui, Bei},
  journal={arXiv preprint arXiv:2403.01886},
  year={2024}
}

@inproceedings{lu-etal-2023-anaphor,
    title = "Anaphor Assisted Document-Level Relation Extraction",
    author = "Lu, Chonggang  and
      Zhang, Richong  and
      Sun, Kai  and
      Kim, Jaein  and
      Zhang, Cunwang  and
      Mao, Yongyi",
    editor = "Bouamor, Houda  and
      Pino, Juan  and
      Bali, Kalika",
    booktitle = "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing",
    month = dec,
    year = "2023",
    address = "Singapore",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.emnlp-main.955",
    doi = "10.18653/v1/2023.emnlp-main.955",
    pages = "15453--15464",
    abstract = "Document-level relation extraction (DocRE) involves identifying relations between entities distributed in multiple sentences within a document. Existing methods focus on building a heterogeneous document graph to model the internal structure of an entity and the external interaction between entities. However, there are two drawbacks in existing methods. On one hand, anaphor plays an important role in reasoning to identify relations between entities but is ignored by these methods. On the other hand, these methods achieve cross-sentence entity interactions implicitly by utilizing a document or sentences as intermediate nodes. Such an approach has difficulties in learning fine-grained interactions between entities across different sentences, resulting in sub-optimal performance. To address these issues, we propose an Anaphor-Assisted (AA) framework for DocRE tasks. Experimental results on the widely-used datasets demonstrate that our model achieves a new state-of-the-art performance.",
}

@article{10.1093/bioinformatics/btae418,
    author = {Yuan, Jianyuan and Zhang, Fengyu and Qiu, Yimeng and Lin, Hongfei and Zhang, Yijia},
    title = "{Document-level biomedical relation extraction via hierarchical tree graph and relation segmentation module}",
    journal = {Bioinformatics},
    volume = {40},
    number = {7},
    pages = {btae418},
    year = {2024},
    month = {06},
    abstract = "{Biomedical relation extraction at the document level (Bio-DocRE) involves extracting relation instances from biomedical texts that span multiple sentences, often containing various entity concepts such as genes, diseases, chemicals, variants, etc. Currently, this task is usually implemented based on graphs or transformers. However, most work directly models entity features to relation prediction, ignoring the effectiveness of entity pair information as an intermediate state for relation prediction. In this article, we decouple this task into a three-stage process to capture sufficient information for improving relation prediction.We propose an innovative framework HTGRS for Bio-DocRE, which constructs a hierarchical tree graph (HTG) to integrate key information sources in the document, achieving relation reasoning based on entity. In addition, inspired by the idea of semantic segmentation, we conceptualize the task as a table-filling problem and develop a relation segmentation (RS) module to enhance relation reasoning based on the entity pair. Extensive experiments on three datasets show that the proposed framework outperforms the state-of-the-art methods and achieves superior performance.Our source code is available at https://github.com/passengeryjy/HTGRS.}",
    issn = {1367-4811},
    doi = {10.1093/bioinformatics/btae418},
    url = {https://doi.org/10.1093/bioinformatics/btae418},
    eprint = {https://academic.oup.com/bioinformatics/article-pdf/40/7/btae418/58463779/btae418.pdf},
}
@article{Zaratiana_Tomeh_Holat_Charnois_2024, title={An Autoregressive Text-to-Graph Framework for Joint Entity and Relation Extraction}, volume={38}, url={https://ojs.aaai.org/index.php/AAAI/article/view/29919}, DOI={10.1609/aaai.v38i17.29919}, abstractNote={In this paper, we propose a novel method for joint entity and relation extraction from unstructured text by framing it as a conditional sequence generation problem. In contrast to conventional generative information extraction models that are left-to-right token-level generators, our approach is \textit{span-based}. It generates a linearized graph where nodes represent text spans and edges represent relation triplets. Our method employs a transformer encoder-decoder architecture with pointing mechanism on a dynamic vocabulary of spans and relation types. Our model can capture the structural characteristics and boundaries of entities and relations through span representations while simultaneously grounding the generated output in the original text thanks to the pointing mechanism. Evaluation on benchmark datasets validates the effectiveness of our approach, demonstrating competitive results. Code is available at https://github.com/urchade/ATG.}, number={17}, journal={Proceedings of the AAAI Conference on Artificial Intelligence}, author={Zaratiana, Urchade and Tomeh, Nadi and Holat, Pierre and Charnois, Thierry}, year={2024}, month={Mar.}, pages={19477-19487} }

@inproceedings{10.5555/3666122.3667241,
author = {Xie, Ming-Kun and Xiao, Jia-Hao and Liu, Hao-Zhe and Niu, Gang and Sugiyama, Masashi and Huang, Sheng-Jun},
title = {Class-distribution-aware pseudo-labeling for semi-supervised multi-label learning},
year = {2024},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Pseudo-labeling has emerged as a popular and effective approach for utilizing unlabeled data. However, in the context of semi-supervised multi-label learning (SSMLL), conventional pseudo-labeling methods encounter difficulties when dealing with instances associated with multiple labels and an unknown label count. These limitations often result in the introduction of false positive labels or the neglect of true positive ones. To overcome these challenges, this paper proposes a novel solution called Class-Aware Pseudo-Labeling (CAP) that performs pseudo-labeling in a class-aware manner. The proposed approach introduces a regularized learning framework incorporating class-aware thresholds, which effectively control the assignment of positive and negative pseudo-labels for each class. Notably, even with a small proportion of labeled examples, our observations demonstrate that the estimated class distribution serves as a reliable approximation. Motivated by this finding, we develop a class-distribution-aware thresholding strategy to ensure the alignment of pseudo-label distribution with the true distribution. The correctness of the estimated class distribution is theoretically verified, and a generalization error bound is provided for our proposed method. Extensive experiments on multiple benchmark datasets confirm the efficacy of CAP in addressing the challenges of SSMLL problems. The implementation is available at https://github.com/milkxie/SSMLL-CAP.},
booktitle = {Proceedings of the 37th International Conference on Neural Information Processing Systems},
articleno = {1119},
numpages = {17},
location = {New Orleans, LA, USA},
series = {NIPS '23}
}